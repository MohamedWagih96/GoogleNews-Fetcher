{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests , nltk , time , os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from pandas import DataFrame\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_driver(driver_name):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "        \n",
    "    if(driver_name == 'GoogleChrome'):\n",
    "        CHROMEDRIVER_PATH = os.getcwd() + '\\chromedriver.exe'\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    elif(driver_name == 'FireFox'):\n",
    "        pass\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_news_scraper(driver , keyword , number_of_pages):\n",
    "    url = 'https://www.google.com/search?q='+keyword+'&hl=en&source=lnms&tbm=nws&sa=X&ved=0ahUKEwi39q3H_ZjdAhXnMewKHbszDXIQ_AUICygC&biw=1536&bih=728&dpr=1.25'\n",
    "    source_list = []\n",
    "    date_list = []\n",
    "    title_list = []\n",
    "    description_list = []\n",
    "    page_count = 1\n",
    "    delay = False\n",
    "    \n",
    "    while(page_count != number_of_pages):\n",
    "        try:\n",
    "            if(delay):\n",
    "                #Some delay in order to not get blocked [Remove it if you don't have too much pages to review]\n",
    "                time.sleep(1)\n",
    "                delay = False\n",
    "                \n",
    "            else:\n",
    "                #Parse the webpage\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.text , 'html5lib')\n",
    "\n",
    "                #Grab the Headline\n",
    "                for title in soup.find_all('h3'):\n",
    "                    title_list.append(title.text)\n",
    "\n",
    "                #Grab the Soruce and Date\n",
    "                for sd in soup.find_all('div',{'class':'slp'}):\n",
    "                    info = sd.text.split(' - ')\n",
    "                    source = info[0]\n",
    "                    source_list.append(source)\n",
    "                    date = info[len(info)-1]\n",
    "                    date_list.append(date)\n",
    "\n",
    "                #Grab the Description\n",
    "                for desc in soup.find_all('div',{'class':'st'}):\n",
    "                    description_list.append(desc.text)\n",
    "\n",
    "                #Get the next page url\n",
    "                driver.get(url)\n",
    "                driver.find_element_by_link_text('Next').click()\n",
    "                url = driver.current_url\n",
    "\n",
    "                page_count += 1\n",
    "                \n",
    "                if(page_count%10 == 0):\n",
    "                    delay = True\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            print(\"No more pages left!\")\n",
    "            driver.quit()\n",
    "            break\n",
    "        \n",
    "            \n",
    "    return title_list , source_list , date_list , description_list , description_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = \"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    ps = analyzer.polarity_scores(text)\n",
    "    \n",
    "    #Positive sentiments\n",
    "    if ps['neg'] <= 0.5 and ps['pos'] > 0:\n",
    "        if ps['pos'] - ps['neg'] >= 0:\n",
    "            sentiment = '1'\n",
    "        elif ps['neu'] >= 0.5:\n",
    "            sentiment = '1'\n",
    "        else:\n",
    "            sentiment = '0'\n",
    "    \n",
    "    #Negative sentiments\n",
    "    elif ps['pos'] <= 0.5 and ps['neg'] > 0:\n",
    "        if ps['neg'] - ps['pos'] >= 0:\n",
    "            sentiment = '0'\n",
    "        elif ps['neu'] <= 0.5:\n",
    "            sentiment = '0'\n",
    "        else:\n",
    "            sentiment = '1'\n",
    "    \n",
    "    else:\n",
    "        sentiment = get_sentiment('Good '+ text)\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_analysis(title_list , description_list):\n",
    "    i = 0\n",
    "    labels_list = []\n",
    "\n",
    "    for sentence in title_list:\n",
    "        sentiment = get_sentiment(sentence)\n",
    "        if(sentiment == '1' or sentiment == '0'):\n",
    "            labels_list.append(sentiment)\n",
    "        elif(sentiment == '?'):\n",
    "            labels_list.append(get_sentiment(description_list[i]))\n",
    "        else:\n",
    "            labels_list.append('?')\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    positive_news_percentage = labels_list.count('1')/len(description_list)*100\n",
    "    negative_news_percentage = labels_list.count('0')/len(description_list)*100\n",
    "    unknown_news_percentage = labels_list.count('?')/len(description_list)*100\n",
    "\n",
    "#     print(\"Positive News Percentage : \" , labels_list.count('1')/len(description_list)*100 , \"%\")\n",
    "#     print(\"Negative News Percentage : \" , labels_list.count('0')/len(description_list)*100 , \"%\")\n",
    "#     print(\"Unknowns : \" , labels_list.count('?') ,'sentences , ' , labels_list.count('?')/len(description_list)*100 , \"%\")\n",
    "\n",
    "    return labels_list , positive_news_percentage , negative_news_percentage , unknown_news_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(file_name , title_list , source_list , date_list , description_list , labels_list):\n",
    "    df = DataFrame({'Headline': title_list , 'Source': source_list , 'Date/Time': date_list , 'Description': description_list , 'Labels': labels_list})\n",
    "    df.to_excel(str(file_name)+'.xlsx' , sheet_name = 'sheet1' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = set_driver('GoogleChrome')\n",
    "title_list , source_list , date_list , description_list , description_list = google_news_scraper(driver , 'canada' , 20)\n",
    "labels_list , positive_news_percentage , negative_news_percentage , unknown_news_percentage = news_analysis(title_list , description_list)\n",
    "save_to_csv('Canada News' , title_list , source_list , date_list , description_list , labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
